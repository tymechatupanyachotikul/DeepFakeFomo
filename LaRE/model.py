import os
import clip
import torch
from torch import nn
from torch.nn import functional as F
from torch.nn import Sequential, Module, Linear
from torchvision.models.resnet import resnet50
from model_base import MultiHeadMapAttention, MultiHeadMapAttentionV2, ResNet, ResidualBlock

type_to_path = {
    'RN50x64': "RN50x64",
    'RN50': "RN50",
}


class Res50Feature(nn.Module):
    def __init__(self, num_class=4):
        super(Res50Feature, self).__init__()
        self.model = resnet50(pretrained=False)
        state_dict = torch.load(
            '/home/petterluo/resnet50-19c8e357.pth')
        self.model.load_state_dict(state_dict)
        print('load ok')
        self.model.fc = nn.Identity()

    def forward(self, image_input, isTrain=False):
        image_feats = self.model(image_input)
        image_feats = image_feats / image_feats.norm(dim=1, keepdim=True)
        return None, image_feats


class LASTED(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50x64"):
        super(LASTED, self).__init__()
        self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.output_layer = Sequential(
        #     nn.Linear(1024, 1280),
        #     nn.GELU(),
        #     nn.Linear(1280, 512),
        # )
        # self.fc = nn.Linear(512, num_class)
        # self.text_input = clip.tokenize(['Real', 'Synthetic'])
        self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        # self.text_input = clip.tokenize(['Real-Photo', 'Synthetic-Photo', 'Real-Painting', 'Synthetic-Painting'])
        # self.text_input = clip.tokenize(['a', 'b', 'c', 'd'])

    def forward(self, image_input, isTrain=True):
        if isTrain:
            logits_per_image, _ = self.clip_model(image_input, self.text_input.to(image_input.device))
            return None, logits_per_image
        else:
            image_feats = self.clip_model.encode_image(image_input)
            image_feats = image_feats / image_feats.norm(dim=1, keepdim=True)
            return None, image_feats


class CLipClassifier(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50"):
        super(CLipClassifier, self).__init__()
        self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.fc = nn.Linear(1024, 2)

    def forward(self, image_input):
        # if isTrain:
        #     logits_per_image, _ = self.clip_model(image_input, self.text_input.to(image_input.device))
        #     return None, logits_per_image
        # else:
        image_feats = self.clip_model.encode_image(image_input)
        image_feats = image_feats / image_feats.norm(dim=1, keepdim=True)
        logits = self.fc(image_feats)
        return logits


class CLipClassifierV2(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50"):
        super(CLipClassifierV2, self).__init__()
        self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.fc = nn.Linear(1024, 2)

    def forward(self, image_input, loss_map=''):
        # if isTrain:
        #     logits_per_image, _ = self.clip_model(image_input, self.text_input.to(image_input.device))
        #     return None, logits_per_image
        # else:
        image_feats = self.clip_model.encode_image(image_input)
        image_feats = image_feats / image_feats.norm(dim=1, keepdim=True)
        logits = self.fc(image_feats)
        return logits


class CLipClassifierV3(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50"):
        super(CLipClassifierV3, self).__init__()
        self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        self.clip_model.visual.attnpool = nn.Identity()
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(2048, 2)

    def forward(self, image_input, loss_map=''):
        # if isTrain:
        #     logits_per_image, _ = self.clip_model(image_input, self.text_input.to(image_input.device))
        #     return None, logits_per_image
        # else:
        image_feats = self.clip_model.encode_image(image_input)
        image_feats = self.pool(image_feats)
        image_feats = torch.flatten(image_feats, 1)
        logits = self.fc(image_feats)
        return logits


class CLipClassifierV4(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50"):
        super(CLipClassifierV4, self).__init__()
        self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        for param in self.clip_model.parameters():
            param.requires_grad = False
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.fc = nn.Linear(1024, 2)

    def forward(self, image_input):
        # if isTrain:
        #     logits_per_image, _ = self.clip_model(image_input, self.text_input.to(image_input.device))
        #     return None, logits_per_image
        # else:
        image_feats = self.clip_model.encode_image(image_input)
        image_feats = image_feats / image_feats.norm(dim=1, keepdim=True)
        logits = self.fc(image_feats)
        return logits


class ResnetBaseline(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50"):
        super(ResnetBaseline, self).__init__()
        self.model = resnet50()
        self.model.load_state_dict(torch.load(
            '/home/petterluo/pretrained_models/resnet50-19c8e357.pth'))
        self.model.fc = nn.Identity()
        # self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.fc = nn.Linear(2048, 2)

    def forward(self, image_input, loss_map):
        # if isTrain:
        #     logits_per_image, _ = self.clip_model(image_input, self.text_input.to(image_input.device))
        #     return None, logits_per_image
        # else:
        image_feats = self.model(image_input)  # bs * 2048
        logits = self.fc(image_feats)
        return logits


class CLipClassifierWMap(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50x64"):
        super(CLipClassifierWMap, self).__init__()
        self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.fc = nn.Linear(1024, 2)
        self.visual_attnpool = self.clip_model.visual.attnpool
        self.clip_model.visual.attnpool = nn.Identity()

        self.conv = nn.Conv2d(4, 1, kernel_size=(1, 1))

    def forward(self, image_input, loss_map):
        image_feats = self.clip_model.encode_image(image_input)
        # image_feats_attnpooled_normed = image_feats_attnpooled
        # print('init:', loss_map.shape)
        loss_map = self.conv(loss_map)  # bs * 1 * 32 * 32
        # print('conv:', loss_map.shape)
        loss_map_pooled = torch.mean(loss_map, dim=1, keepdim=True)  # bs * 1 * 32 * 32
        loss_map_pooled = F.adaptive_avg_pool2d(loss_map_pooled, (7, 7))
        spatial_weights = torch.sigmoid(loss_map_pooled)  # bs * 1 * 7 * 7

        feats_with_weights = image_feats * spatial_weights
        # feats_with_weights_pooled = F.adaptive_avg_pool2d(feats_with_weights, (1, 1)).squeeze() # bs * 2048
        image_feats = feats_with_weights
        image_feats_attnpooled = self.visual_attnpool(image_feats)

        final_feats = image_feats_attnpooled
        # print(image_feats.shape)
        # print(loss_map.shape)
        logits = self.fc(final_feats)
        return logits


class CLipClassifierWMapV2(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50x64"):
        super(CLipClassifierWMapV2, self).__init__()
        self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.fc = nn.Linear(1024, 2)
        self.visual_attnpool = self.clip_model.visual.attnpool
        self.clip_model.visual.attnpool = nn.Identity()

        self.conv = nn.Conv2d(4, 1, kernel_size=(1, 1))

    def forward(self, image_input, loss_map):
        image_feats = self.clip_model.encode_image(image_input)
        # image_feats_attnpooled_normed = image_feats_attnpooled
        # print('init:', loss_map.shape)
        loss_map = self.conv(loss_map)  # bs * 1 * 32 * 32
        # print('conv:', loss_map.shape)
        loss_map_pooled = torch.mean(loss_map, dim=1, keepdim=True)  # bs * 1 * 32 * 32
        loss_map_pooled = F.adaptive_avg_pool2d(loss_map_pooled, (7, 7))
        spatial_weights = torch.sigmoid(loss_map_pooled)  # bs * 1 * 7 * 7

        feats_with_weights = image_feats * spatial_weights
        image_feats_attnpooled = self.visual_attnpool(image_feats + feats_with_weights)
        logits = self.fc(image_feats_attnpooled)
        return logits


class Upsample(nn.Module):
    def __init__(self, in_channels, out_channels, with_conv):
        super().__init__()
        self.with_conv = with_conv
        if self.with_conv:
            self.conv = torch.nn.Conv2d(in_channels,
                                        out_channels,
                                        kernel_size=3,
                                        stride=1,
                                        padding=1)

    def forward(self, x):
        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode="nearest")
        if self.with_conv:
            x = self.conv(x)
        return x


class Downsample(nn.Module):
    def __init__(self, in_channels, out_channels, with_conv):
        super().__init__()
        self.with_conv = with_conv
        if self.with_conv:
            # no asymmetric padding in torch conv, must do it ourselves
            self.conv = torch.nn.Conv2d(in_channels,
                                        out_channels,
                                        kernel_size=3,
                                        stride=2,
                                        padding=0)

    def forward(self, x):
        if self.with_conv:
            pad = (0, 1, 0, 1)
            x = torch.nn.functional.pad(x, pad, mode="constant", value=0)
            x = self.conv(x)
        else:
            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)
        return x


class CLipClassifierWMapV3(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50x64"):
        super(CLipClassifierWMapV3, self).__init__()
        self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.fc = nn.Linear(512, 2)
        self.visual_attnpool = self.clip_model.visual.attnpool
        self.clip_model.visual.attnpool = nn.Identity()

        self.up_feature_map = Upsample(2048, 512, with_conv=True)
        self.down_loss_map = Downsample(4, 1, with_conv=True)

        self.conv = nn.Conv2d(4, 1, kernel_size=(1, 1))

    def forward(self, image_input, loss_map):
        # loss_map bs * 4 * 32 * 32
        image_feats = self.clip_model.encode_image(image_input)  # bs * 2048 * 7 * 7
        up_image_feats = self.up_feature_map(image_feats)  # bs * 512 * 14 * 14

        down_loss_map = self.down_loss_map(loss_map)  # bs * 16 * 16 * 16
        down_loss_map = F.adaptive_avg_pool2d(down_loss_map, (14, 14))  # bs * 1 * 14 * 14

        up_image_feats = up_image_feats * torch.sigmoid(down_loss_map)
        global_feats = F.adaptive_avg_pool2d(up_image_feats, (1, 1)).squeeze()  # bs * 512

        logits = self.fc(global_feats)
        return logits


def forward(self, x):
    def stem(x):
        x = self.relu1(self.bn1(self.conv1(x)))
        x = self.relu2(self.bn2(self.conv2(x)))
        x = self.relu3(self.bn3(self.conv3(x)))
        x = self.avgpool(x)
        return x

    x = x.type(self.conv1.weight.dtype)
    x = stem(x)
    x = self.layer1(x)
    x = self.layer2(x)
    x_l3 = self.layer3(x)
    x = self.layer4(x_l3)
    x = self.attnpool(x)

    return x, x_l3


def encode_image(self, image):
    return self.visual(self.visual, image.type(self.dtype))


class CLipClassifierWMapV4(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50x64"):
        super(CLipClassifierWMapV4, self).__init__()
        self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.fc = nn.Linear(1024 + 1024, 2)
        # self.visual_attnpool = self.clip_model.visual.attnpool
        self.clip_model.visual.forward = forward
        self.clip_model.encode_image = encode_image

        self.conv = nn.Conv2d(4, 1, kernel_size=(1, 1))

    def forward(self, image_input, loss_map):
        # loss_map bs * 4 * 32 * 32
        image_feats, block3_feats = self.clip_model.encode_image(self.clip_model, image_input)
        # block3 bs * 1024 * 14 * 14
        # print(block3_feats.shape)
        # up_image_feats = self.up_feature_map(image_feats)  # bs * 1024 * 14 * 14

        loss_map = F.adaptive_avg_pool2d(loss_map, (14, 14))  # bs * 4 * 14 * 14
        pooled_loss_map = self.conv(loss_map)  # bs * 1 * 14 * 14
        up_image_feats = block3_feats * torch.sigmoid(pooled_loss_map)
        global_feats = F.adaptive_avg_pool2d(up_image_feats, (1, 1)).squeeze()  # bs * 512

        logits = self.fc(torch.cat([image_feats, global_feats], dim=1))
        return logits


class CLipClassifierWMapV5(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50x64"):
        super(CLipClassifierWMapV5, self).__init__()
        self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.fc = nn.Linear(1024 + 1024, 2)
        # self.visual_attnpool = self.clip_model.visual.attnpool
        # Monkey patch
        self.clip_model.visual.forward = forward
        self.clip_model.encode_image = encode_image

        self.conv = nn.Conv2d(4, 8, kernel_size=(1, 1))  # for 8 heads
        self.attn_pool = MultiHeadMapAttention()

    def forward(self, image_input, loss_map):
        # loss_map bs * 4 * 32 * 32
        image_feats, block3_feats = self.clip_model.encode_image(self.clip_model, image_input)
        # block3 bs * 1024 * 14 * 14
        # print(block3_feats.shape)
        # up_image_feats = self.up_feature_map(image_feats)  # bs * 1024 * 14 * 14

        loss_map = F.adaptive_avg_pool2d(loss_map, (14, 14))  # bs * 4 * 14 * 14
        pooled_loss_map = self.conv(loss_map)  # bs * 8 * 14 * 14

        pooled_block3_feats = self.attn_pool(block3_feats, pooled_loss_map)  # bs * 1024
        # print(pooled_block3_feats.shape)
        # up_image_feats = block3_feats * torch.sigmoid(pooled_loss_map)
        # global_feats = F.adaptive_avg_pool2d(up_image_feats, (1, 1)).squeeze()  # bs * 512
        logits = self.fc(torch.cat([image_feats, pooled_block3_feats], dim=1))
        return logits


class CLipClassifierWMapV6(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50x64"):
        super(CLipClassifierWMapV6, self).__init__()
        self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.fc = nn.Linear(1024 + 1024 + 1024, 2)
        # self.visual_attnpool = self.clip_model.visual.attnpool
        # Monkey patch
        self.clip_model.visual.forward = forward
        self.clip_model.encode_image = encode_image

        self.conv = nn.Conv2d(4, 8, kernel_size=(1, 1))  # for 8 heads
        self.conv_align = nn.Conv2d(4, 1024, kernel_size=(1, 1))
        self.attn_pool = MultiHeadMapAttention()
        self.channel_align = ChannelAlignLayer(4, 128, 1024)

    def forward(self, image_input, loss_map):
        # loss_map bs * 4 * 32 * 32
        image_feats, block3_feats = self.clip_model.encode_image(self.clip_model, image_input)
        # block3 bs * 1024 * 14 * 14
        aligned_loss_map = F.adaptive_avg_pool2d(loss_map, (14, 14))  # bs * 4 * 14 * 14
        pooled_loss_map = self.conv(aligned_loss_map)  # bs * 8 * 14 * 14
        pooled_block3_feats = self.attn_pool(block3_feats, pooled_loss_map)  # bs * 1024

        channel_weighted_feats = self.channel_align(block3_feats, loss_map)  # bs * 1024
        logits = self.fc(torch.cat([image_feats, pooled_block3_feats, channel_weighted_feats], dim=1))
        return logits


class CLipClassifierOnlyMapV1(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50x64"):
        super(CLipClassifierOnlyMapV1, self).__init__()
        # self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.model = ResNet(ResidualBlock, [2, 2, 2], 2)

    def forward(self, image_input, loss_map):
        # loss_map bs * 4 * 32 * 32
        logits = self.model(loss_map)
        return logits


class CLipClassifierOnlyMapV2(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50x64"):
        super(CLipClassifierOnlyMapV2, self).__init__()
        # self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.model = ResNet(ResidualBlock, [2, 2, 2], 2)

    def forward(self, image_input, loss_map):
        # loss_map bs * 4 * 32 * 32
        logits = self.model(loss_map)
        return logits


class CLipClassifierWMapV7(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50x64"):
        super(CLipClassifierWMapV7, self).__init__()
        self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.fc = nn.Linear(1024 + 1024 + 1024 + 1024, 2)
        # self.visual_attnpool = self.clip_model.visual.attnpool
        # Monkey patch
        self.clip_model.visual.forward = forward
        self.clip_model.encode_image = encode_image

        self.conv = nn.Conv2d(4, 1024, kernel_size=(1, 1))  # for 8 heads
        self.conv_align = nn.Conv2d(4, 1024, kernel_size=(1, 1))
        # self.attn_pool = MultiHeadMapAttention()
        self.channel_align = ChannelAlignLayer(4, 128, 1024)
        self.attn_pool = MultiHeadMapAttentionV2()

    def forward(self, image_input, loss_map):
        # loss_map bs * 4 * 32 * 32
        image_feats, block3_feats = self.clip_model.encode_image(self.clip_model, image_input)
        # block3 bs * 1024 * 14 * 14
        aligned_loss_map = F.adaptive_avg_pool2d(loss_map, (14, 14))  # bs * 4 * 14 * 14
        upscaled_loss_map = self.conv(aligned_loss_map)  # bs * 1024 * 14 * 14

        refined_feats = self.attn_pool(block3_feats, upscaled_loss_map)  # bs * 2048

        channel_weighted_feats = self.channel_align(block3_feats, loss_map)  # bs * 1024
        logits = self.fc(torch.cat([image_feats, refined_feats, channel_weighted_feats], dim=1))
        return logits


class PoolWithMap(nn.Module):
    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):
        super().__init__()
        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)
        self.num_heads = num_heads

    def forward(self, x, loss_map):
        # x bs * 1024 * 14 * 14
        # loss_map bs * 1024 * 14 * 14
        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC
        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC

        loss_map = loss_map.flatten(start_dim=2).permute(2, 0, 1)
        loss_map = torch.cat([loss_map.mean(dim=0, keepdim=True), loss_map], dim=0)
        loss_map = loss_map + self.positional_embedding[:, None, :].to(loss_map.dtype)  # (HW+1)NC

        queries = torch.cat([x[:1], loss_map[:1]], dim=0)  # 2 * N * C

        x, _ = F.multi_head_attention_forward(
            query=x[:1], key=x, value=x,
            embed_dim_to_check=x.shape[-1],
            num_heads=self.num_heads,
            q_proj_weight=self.q_proj.weight,
            k_proj_weight=self.k_proj.weight,
            v_proj_weight=self.v_proj.weight,
            in_proj_weight=None,
            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
            bias_k=None,
            bias_v=None,
            add_zero_attn=False,
            dropout_p=0,
            out_proj_weight=self.c_proj.weight,
            out_proj_bias=self.c_proj.bias,
            use_separate_proj_weight=True,
            training=self.training,
            need_weights=False
        )
        return x.squeeze(0)


class ChannelAlignLayer(nn.Module):
    def __init__(self, in_dim, mid_dim, out_dim):
        super(ChannelAlignLayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_dim, mid_dim, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(mid_dim, out_dim, bias=False),
            nn.Sigmoid()
        )

    def forward(self, feature_map, loss_map):
        b, c, _, _ = feature_map.size()
        pooled_feature_map = self.avg_pool(feature_map).squeeze()  # bs * 1024
        pooled_loss_map = self.avg_pool(loss_map).squeeze()  # bs * 1024
        weights = self.fc(pooled_loss_map)
        out = pooled_feature_map * weights
        return out


class CLipClassifierWMapBaseline(nn.Module):
    def __init__(self, num_class=4, clip_type="RN50x64"):
        super(CLipClassifierWMapBaseline, self).__init__()
        self.clip_model, self.preprocess = clip.load(type_to_path[clip_type], device='cpu', jit=False)
        # self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        self.fc = nn.Linear(2048, 2)
        self.clip_model.visual.attnpool = nn.Identity()
        self.conv = nn.Conv2d(4, 1, kernel_size=(1, 1))

    def forward(self, image_input, loss_map):
        image_feats = self.clip_model.encode_image(image_input)
        loss_map = self.conv(loss_map)  # bs * 1 * 32 * 32
        loss_map_pooled = torch.mean(loss_map, dim=1, keepdim=True)  # bs * 1 * 32 * 32
        loss_map_pooled = F.adaptive_avg_pool2d(loss_map_pooled, (7, 7))
        spatial_weights = torch.sigmoid(loss_map_pooled)  # bs * 1 * 7 * 7

        feats_with_weights = image_feats * spatial_weights
        final_feats = F.adaptive_avg_pool2d(feats_with_weights, (1, 1)).squeeze()
        logits = self.fc(final_feats)
        return logits


class LASTEDWLoss(nn.Module):
    def __init__(self, num_class=4):
        super(LASTEDWLoss, self).__init__()
        self.clip_model, self.preprocess = clip.load(
            "/home/petterluo/clip_models/RN50x64.pt", device='cpu', jit=False)
        # self.output_layer = Sequential(
        #     nn.Linear(1024, 1280),
        #     nn.GELU(),
        #     nn.Linear(1280, 512),
        # )
        # self.fc = nn.Linear(512, num_class)
        # self.text_input = clip.tokenize(['Real', 'Synthetic'])
        self.text_input = clip.tokenize(['Real Photo', 'Synthetic Photo', 'Real Painting', 'Synthetic Painting'])
        # self.text_input = clip.tokenize(['Real-Photo', 'Synthetic-Photo', 'Real-Painting', 'Synthetic-Painting'])
        # self.text_input = clip.tokenize(['a', 'b', 'c', 'd'])

    def forward(self, image_input, isTrain=True):
        if isTrain:
            logits_per_image, _ = self.clip_model(image_input, self.text_input.to(image_input.device))
            return None, logits_per_image
        else:
            image_feats = self.clip_model.encode_image(image_input)
            image_feats = image_feats / image_feats.norm(dim=1, keepdim=True)
            return None, image_feats


class CLipClassifierWMapViT(nn.Module):
    def __init__(self, num_class=2, clip_type="ViT-L/14"):
        super(CLipClassifierWMapViT, self).__init__()
        self.clip_model, _ = clip.load(clip_type, device='cpu', jit=False)
        self.feat_dim = self.clip_model.visual.proj.shape[1]  # 512 for ViT-B/32, 768 for ViT-L/14

        self.fc = nn.Linear(self.feat_dim * 3, num_class)

        # Project loss map channels to match ViT spatial attention
        self.conv_map = nn.Conv2d(4, 8, kernel_size=1)
        self.channel_align = ChannelAlignLayer(in_dim=4, mid_dim=128, out_dim=self.feat_dim)

        # Attention pooling adapted to feat_dim
        self.attn_pool = MultiHeadMapAttention(feat_dim=self.feat_dim, num_heads=8)

    def forward(self, image_input, loss_map):
        # CLIP's ViT encode_image gives [B, feat_dim]
        image_feats = self.clip_model.encode_image(image_input)  # [B, feat_dim]

        # Simulate intermediate block3_feats as a spatial map
        block3_feats = image_feats.unsqueeze(-1).unsqueeze(-1).expand(-1, self.feat_dim, 16, 16)

        spatial_dim = int(block3_feats.size(-1))  # e.g., 16 for ViT-L/14
        aligned_loss_map = F.adaptive_avg_pool2d(loss_map, (spatial_dim, spatial_dim))  # [B, 4, 16, 16]
        pooled_loss_map = self.conv_map(aligned_loss_map)             # [B, 8, 16, 16]

        pooled_block3_feats = self.attn_pool(block3_feats, pooled_loss_map)   # [B, feat_dim]
        channel_weighted_feats = self.channel_align(block3_feats, loss_map)   # [B, feat_dim]

        logits = self.fc(torch.cat([image_feats, pooled_block3_feats, channel_weighted_feats], dim=1))
        return logits

class CLipClassifierWMapConvNeXt(nn.Module):
    def __init__(self, num_class=2, clip_type="convnext_large_d"):
        super(CLipClassifierWMapConvNeXt, self).__init__()

        # Load OpenCLIP ConvNeXt model (e.g., convnext_base, convnext_large)
        self.clip_model, _, preprocess = open_clip.create_model_and_transforms(
            model_name=clip_type,
            pretrained='laion2b_s26b_b102k_augreg',
            device='cpu'  # or 'cuda' if loading directly to GPU
        )
        self.preprocess = preprocess
        self.feat_dim = 768 #self.clip_model.embed_dim  # e.g., 512 for convnext_base

        # Final classifier
        self.fc = nn.Linear(self.feat_dim * 3, num_class)

        # Loss-map processing
        self.conv_map = nn.Conv2d(4, 8, kernel_size=1)
        self.channel_align = ChannelAlignLayer(in_dim=4, mid_dim=128, out_dim=self.feat_dim)
        self.attn_pool = MultiHeadMapAttention(feat_dim=self.feat_dim, num_heads=8)

    def forward(self, image_input, loss_map):
        # Global pooled CLIP features
        image_feats = self.clip_model.encode_image(image_input)  # [B, feat_dim]

        # Simulate spatial feature map (no intermediate features in OpenCLIP)
        spatial_size = 14  # can also be 7 or 16 depending on patch resolution
        block3_feats = image_feats.unsqueeze(-1).unsqueeze(-1).expand(-1, self.feat_dim, spatial_size, spatial_size)

        # Prepare loss map
        aligned_loss_map = F.adaptive_avg_pool2d(loss_map, (spatial_size, spatial_size))  # [B, 4, H, W]
        pooled_loss_map = self.conv_map(aligned_loss_map)  # [B, 8, H, W]

        pooled_block3_feats = self.attn_pool(block3_feats, pooled_loss_map)  # [B, feat_dim]
        channel_weighted_feats = self.channel_align(block3_feats, loss_map)  # [B, feat_dim]

        # Final classifier
        logits = self.fc(torch.cat([image_feats, pooled_block3_feats, channel_weighted_feats], dim=1))
        return logits

class MultiHeadMapAttention(nn.Module):
    def __init__(self, spatial_dim=16, feat_dim=512, num_heads=8):
        super().__init__()
        self.num_tokens = spatial_dim * spatial_dim + 1
        self.positional_embedding = nn.Parameter(torch.randn(self.num_tokens, feat_dim) / feat_dim**0.5)
        #self.positional_embedding = nn.Parameter(torch.randn(257, feat_dim) / feat_dim**0.5)
        self.k_proj = nn.Linear(feat_dim, feat_dim)
        self.q_proj = nn.Linear(feat_dim, feat_dim)
        self.v_proj = nn.Linear(feat_dim, feat_dim)
        self.c_proj = nn.Linear(feat_dim, feat_dim)
        self.num_heads = num_heads

    def forward(self, x, loss_map):
        B, C, H, W = x.shape  # [B, C, 16, 16]
        x = x.flatten(start_dim=2).permute(2, 0, 1)  # [HW, B, C]
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # [HW+1, B, C]
        x = x + self.positional_embedding[:x.size(0), None, :].to(x.dtype)

        query = x[:1]  # CLS token
        x, _ = F.multi_head_attention_forward(
            query=query, key=x, value=x,
            embed_dim_to_check=C,
            num_heads=self.num_heads,
            q_proj_weight=self.q_proj.weight,
            k_proj_weight=self.k_proj.weight,
            v_proj_weight=self.v_proj.weight,
            in_proj_weight=None,
            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
            bias_k=None,
            bias_v=None,
            add_zero_attn=False,
            dropout_p=0,
            out_proj_weight=self.c_proj.weight,
            out_proj_bias=self.c_proj.bias,
            use_separate_proj_weight=True,
            training=self.training,
            need_weights=False
        )
        return x.squeeze(0)

if __name__ == '__main__':
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'

    model = LASTED().cuda()
    model.eval()

    params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print('Params: %.2f' % (params / (1024 ** 2)))

    x = torch.zeros([4, 3, 448, 448]).cuda()
    _, logits = model(x)
    print(logits.shape)
